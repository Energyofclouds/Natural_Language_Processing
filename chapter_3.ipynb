{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPGJkGMuLG63ENMXr58QKIb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# 3.1. 3 신경망에서의 단어 처리"],"metadata":{"id":"bRKtYG__Tcrn"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"XlodIZhkNcDv"},"outputs":[],"source":["import numpy as np\n","\n","C = np.array([[1, 0, 0, 0, 0, 0, 0]])\n","W = np.random.randn(7,3)\n","h = np.matmul(C,W)\n","\n","print(h)"]},{"cell_type":"code","source":["from google.colab import drive\n","\n","drive.mount('/content/drive')"],"metadata":{"id":"E0VdhSN3O0QG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Colab Notebooks\n","from common.layers import MatMul"],"metadata":{"id":"sZ0kwLvjSVNP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from common.layers import MatMul\n","\n","c = np.array([[1, 0, 0, 0, 0, 0, 0]])\n","W = np.random.randn(7,3)\n","layer = MatMul(W)\n","h = layer.forward(c)\n","print(h)"],"metadata":{"id":"96sI2yW0SiRB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3. 2. 1 CBOW 모델의 추론"],"metadata":{"id":"lAap18R9TpYZ"}},{"cell_type":"code","source":["import numpy as np\n","\n","from common.layers import MatMul\n","\n","c0 = np.array([[1, 0, 0, 0, 0, 0, 0]])\n","c1 = np.array([[0, 0, 1, 0, 0, 0, 0]])\n","\n","W_in = np.random.randn(7,3)\n","W_out = np.random.randn(3,7)\n","\n","in_layer0 = MatMul(W_in)\n","in_layer1 = MatMul(W_in)\n","out_layer = MatMul(W_out)\n","\n","h0 = in_layer0.forward(c0)\n","h1 = in_layer1.forward(c1)\n","h = 0.5 * (h0 + h1)\n","s = out_layer.forward(h)\n","\n","print(s)"],"metadata":{"id":"smnbsNnhTv3n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from common.util import preprocess\n","\n","text = 'You say goodbye and I say hello.'\n","\n","corpus, word_to_id, id_to_word = preprocess(text)\n","print(corpus)\n","\n","print(id_to_word)"],"metadata":{"id":"ZzqP14V6XL5I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from common.util import *\n","\n","text = 'You say goodbye and I say hello.'\n","corpus, word_to_id, id_to_word = preprocess(text)\n","\n","contexts, target = create_contexts_target(corpus, window_size = 1)\n","\n","vocab_size = len(word_to_id)\n","target = convert_one_hot(target, vocab_size)\n","contexts = convert_one_hot(contexts, vocab_size)"],"metadata":{"id":"ff9AXR7tXcfT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","from common.layers import *\n","\n","class SimpleCBOW:\n","    def __init__(self, vocab_size, hidden_size):\n","        V, H = vocab_size, hidden_size\n","\n","\n","        W_in = 0.01 * np.random.randn(V, H).astype('f')\n","        W_out = 0.01 * np.random.randn(H,V).astype('f')\n","\n","\n","        self.in_layer0 = MatMul(W_in)\n","        self.in_layer1 = MatMul(W_in)\n","        self.out_layer = MatMul(W_out)\n","        self.loss_layer = SoftmaxWithLoss()\n","\n","\n","        layers = [self.in_layer0, self.in_layer1, self.out_layer]\n","        self.params, self.grads = [], []\n","\n","        for layer in layers:\n","            self.params += layer.params\n","            self.grads += layer.grads\n","\n","\n","\n","        self.word_vecs_in = W_in\n","        self.word_vecs_out = W_out.T\n","        self.word_vecs_in_out = np.dot(W_in,W_out)\n","    def forward(self, contexts, target):\n","        h0 = self.in_layer0.forward(contexts[:,0])\n","        h1 = self.in_layer1.forward(contexts[:,1])\n","        h = (h0+h1)*0.5\n","        score = self.out_layer.forward(h)\n","        loss = self.loss_layer.forward(score, target)\n","        return loss\n","\n","\n","    def backward(self, dout=1):\n","        ds = self.loss_layer.backward(dout)\n","        da = self.out_layer.backward(ds)\n","        da *= 0.5\n","        self.in_layer1.backward(da)\n","        self.in_layer0.backward(da)\n","        return None\n","\n","\n","\n"],"metadata":{"id":"lr03KyarX2eT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from common.trainer import Trainer\n","from common.optimizer import Adam\n","from common.util import *\n","\n","\n","window_size = 1\n","hidden_size = 5\n","batch_size = 3\n","\n","max_epoch = 1000\n","\n","text = 'You say goodbay and I say hello.'\n","corpus, word_to_id, id_to_word = preprocess(text)\n","\n","\n","vocab_size = len(word_to_id)\n","contexts, target = create_contexts_target(corpus, window_size)\n","target = convert_one_hot(target, vocab_size)\n","contexts = convert_one_hot(contexts, vocab_size)\n","\n","model = SimpleCBOW(vocab_size, hidden_size)\n","\n","optimizer = Adam()\n","\n","trainer = Trainer(model, optimizer)\n","\n","trainer.fit(contexts, target, max_epoch, batch_size)\n","trainer.plot()"],"metadata":{"id":"eTZVeSStaeZW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["word_vecs_in = np.round(model.word_vecs_in, 7)\n","word_vecs_out = np.round(model.word_vecs_out, 7)\n","word_vecs_in_out = np.round(model.word_vecs_in_out, 6)\n","\n","print(\"입력층만 사용\")\n","for word_id, word in id_to_word.items():\n","    print(\"%10s\" %word, end='')\n","    print(word_vecs_in[word_id])\n","print(\"-\"*100)\n","\n","print(\"출력층만 사용\")\n","for word_id, word in id_to_word.items():\n","    print(\"%10s\" %word, end='')\n","    print(word_vecs_out[word_id])\n","print(\"-\"*100)\n","\n","print(\"입력, 출력층 사용\")\n","for word_id, word in id_to_word.items():\n","    print(\"%10s\" %word, end='')\n","    print(word_vecs_in_out[word_id])\n","print(\"-\"*100)"],"metadata":{"id":"cFFTC1P-bdmi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# skip - Gram"],"metadata":{"id":"XN3SafcG7ZL4"}},{"cell_type":"code","source":["import numpy as np\n","from common.layers import MatMul, SoftmaxWithLoss\n","\n","\n","class SimpleSkipGram:\n","    def __init__(self, vocab_size, hidden_size):\n","        V, H = vocab_size, hidden_size\n","\n","        \n","        W_in = 0.01 * np.random.randn(V, H).astype('f')\n","        W_out = 0.01 * np.random.randn(H, V).astype('f')\n","\n","        \n","        self.in_layer = MatMul(W_in)\n","        self.out_layer = MatMul(W_out)\n","        self.loss_layer1 = SoftmaxWithLoss()\n","        self.loss_layer2 = SoftmaxWithLoss()\n","\n","        \n","        layers = [self.in_layer, self.out_layer]\n","        self.params, self.grads = [], []\n","        for layer in layers:\n","            self.params += layer.params\n","            self.grads += layer.grads\n","\n","        \n","        self.word_vecs_in = W_in\n","        self.word_vecs_out = W_out.T\n","        self.word_vecs_in_out = np.dot(W_in,W_out)\n","\n","    def forward(self, contexts, target):\n","        h = self.in_layer.forward(target)\n","        s = self.out_layer.forward(h)\n","        l1 = self.loss_layer1.forward(s, contexts[:, 0])\n","        l2 = self.loss_layer2.forward(s, contexts[:, 1])\n","        loss = l1 + l2\n","        return loss\n","\n","    def backward(self, dout=1):\n","        dl1 = self.loss_layer1.backward(dout)\n","        dl2 = self.loss_layer2.backward(dout)\n","        ds = dl1 + dl2\n","        dh = self.out_layer.backward(ds)\n","        self.in_layer.backward(dh)\n","        return None\n"],"metadata":{"id":"pGQIZXCv7Vot"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from common.trainer import Trainer\n","from common.optimizer import Adam\n","from common.util import *\n","\n","\n","window_size = 1\n","hidden_size = 5\n","batch_size = 3\n","\n","max_epoch = 1000\n","\n","text = 'You say goodbay and I say hello.'\n","corpus, word_to_id, id_to_word = preprocess(text)\n","\n","\n","vocab_size = len(word_to_id)\n","contexts, target = create_contexts_target(corpus, window_size)\n","target = convert_one_hot(target, vocab_size)\n","contexts = convert_one_hot(contexts, vocab_size)\n","\n","model = SimpleSkipGram(vocab_size, hidden_size)\n","\n","optimizer = Adam()\n","\n","trainer = Trainer(model, optimizer)\n","\n","trainer.fit(contexts, target, max_epoch, batch_size)\n","trainer.plot()"],"metadata":{"id":"XONrMb3-7YRE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["word_vecs_in = np.round(model.word_vecs_in, 7)\n","word_vecs_out = np.round(model.word_vecs_out, 7)\n","word_vecs_in_out = np.round(model.word_vecs_in_out, 6)\n","\n","print(\"입력층만 사용\")\n","for word_id, word in id_to_word.items():\n","    print(\"%10s\" %word, end='')\n","    print(word_vecs_in[word_id])\n","print(\"-\"*100)\n","\n","print(\"출력층만 사용\")\n","for word_id, word in id_to_word.items():\n","    print(\"%10s\" %word, end='')\n","    print(word_vecs_out[word_id])\n","print(\"-\"*100)\n","\n","print(\"입력, 출력층 사용\")\n","for word_id, word in id_to_word.items():\n","    print(\"%10s\" %word, end='')\n","    print(word_vecs_in_out[word_id])\n","print(\"-\"*100)"],"metadata":{"id":"DEA6q5DC_PsQ"},"execution_count":null,"outputs":[]}]}